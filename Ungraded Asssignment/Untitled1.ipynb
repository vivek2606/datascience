{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Flux with Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 15   1   0\n",
       "  0  15   2\n",
       "  0   2  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9\n",
      "\n",
      "Confusion Matrix:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: logitcrossentropy, normalise, onecold, onehotbatch\n",
    "using Statistics: mean\n",
    "using Parameters: @with_kw\n",
    "\n",
    "@with_kw mutable struct Args\n",
    "    lr::Float64 = 0.5\n",
    "    repeat::Int = 110\n",
    "end\n",
    "\n",
    "function get_processed_data(args)\n",
    "    labels = Flux.Data.Iris.labels()\n",
    "    features = Flux.Data.Iris.features()\n",
    "\n",
    "    # Subract mean, divide by std dev for normed mean of 0 and std dev of 1.\n",
    "    normed_features = normalise(features, dims=2)\n",
    "\n",
    "    klasses = sort(unique(labels))\n",
    "    onehot_labels = onehotbatch(labels, klasses)\n",
    "\n",
    "    # Split into training and test sets, 2/3 for training, 1/3 for test.\n",
    "    train_indices = [1:3:150 ; 2:3:150]\n",
    "\n",
    "    X_train = normed_features[:, train_indices]\n",
    "    y_train = onehot_labels[:, train_indices]\n",
    "\n",
    "    X_test = normed_features[:, 3:3:150]\n",
    "    y_test = onehot_labels[:, 3:3:150]\n",
    "\n",
    "    #repeat the data `args.repeat` times\n",
    "    train_data = Iterators.repeated((X_train, y_train), args.repeat)\n",
    "    test_data = (X_test,y_test)\n",
    "\n",
    "    return train_data, test_data\n",
    "end\n",
    "\n",
    "# Accuracy Function\n",
    "accuracy(x, y, model) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "# Function to build confusion matrix\n",
    "function confusion_matrix(X, y, model)\n",
    "    ŷ = onehotbatch(onecold(model(X)), 1:3)\n",
    "    y * transpose(ŷ)\n",
    "end\n",
    "\n",
    "function train(; kws...)\n",
    "    # Initialize hyperparameter arguments\n",
    "    args = Args(; kws...)\t\n",
    "\n",
    "    #Loading processed data\n",
    "    train_data, test_data = get_processed_data(args)\n",
    "\n",
    "    # Declare model taking 4 features as inputs and outputting 3 probabiltiies, \n",
    "    # one for each species of iris.\n",
    "    model = Chain(Dense(4, 3))\n",
    "\t\n",
    "    # Defining loss function to be used in training\n",
    "    # For numerical stability, we use here logitcrossentropy\n",
    "    loss(x, y) = logitcrossentropy(model(x), y)\n",
    "\t\n",
    "    # Training\n",
    "    # Gradient descent optimiser with learning rate `args.lr`\n",
    "    optimiser = Descent(args.lr)\n",
    "\n",
    "    println(\"Starting training.\")\n",
    "    Flux.train!(loss, params(model), train_data, optimiser)\n",
    "\t\n",
    "    return model, test_data\n",
    "end\n",
    "\n",
    "function test(model, test)\n",
    "    # Testing model performance on test data \n",
    "    X_test, y_test = test\n",
    "    accuracy_score = accuracy(X_test, y_test, model)\n",
    "\n",
    "    println(\"\\nAccuracy: $accuracy_score\")\n",
    "\n",
    "    # Sanity check.\n",
    "    @assert accuracy_score > 0.8\n",
    "\n",
    "    # To avoid confusion, here is the definition of a Confusion Matrix: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    println(\"\\nConfusion Matrix:\\n\")\n",
    "    display(confusion_matrix(X_test, y_test, model))\n",
    "end\n",
    "\n",
    "cd(@__DIR__)\n",
    "model, test_data = train()\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN using MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data set\n",
      "└ @ Main In[1]:90\n",
      "┌ Info: Building model...\n",
      "└ @ Main In[1]:95\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[1]:119\n",
      "┌ Info: [1]: Test accuracy: 0.9822\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [2]: Test accuracy: 0.9863\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [3]: Test accuracy: 0.9883\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [4]: Test accuracy: 0.9896\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [5]: Test accuracy: 0.9894\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [6]: Test accuracy: 0.9872\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [7]: Test accuracy: 0.9902\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [8]: Test accuracy: 0.9887\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [9]: Test accuracy: 0.9894\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [10]: Test accuracy: 0.9880\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [11]: Test accuracy: 0.9896\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [12]: Test accuracy: 0.9898\n",
      "└ @ Main In[1]:135\n",
      "┌ Warning:  -> Haven't improved in a while, dropping learning rate to 0.00030000000000000003!\n",
      "└ @ Main In[1]:153\n",
      "┌ Info: [13]: Test accuracy: 0.9923\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [14]: Test accuracy: 0.9926\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [15]: Test accuracy: 0.9924\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [16]: Test accuracy: 0.9918\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [17]: Test accuracy: 0.9928\n",
      "└ @ Main In[1]:135\n",
      "┌ Info:  -> New best accuracy! Saving model out to mnist_conv.bson\n",
      "└ @ Main In[1]:144\n",
      "┌ Info: [18]: Test accuracy: 0.9925\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [19]: Test accuracy: 0.9924\n",
      "└ @ Main In[1]:135\n",
      "┌ Info: [20]: Test accuracy: 0.9925\n",
      "└ @ Main In[1]:135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy(test_set..., model) = 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9928"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifies MNIST digits with a convolutional network.\n",
    "# Writes out saved model to the file \"mnist_conv.bson\".\n",
    "# Demonstrates basic model construction, training, saving,\n",
    "# conditional early-exit, and learning rate scheduling.\n",
    "#\n",
    "# This model, while simple, should hit around 99% test\n",
    "# accuracy after training for approximately 20 epochs.\n",
    "\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, logitcrossentropy\n",
    "using Base.Iterators: partition\n",
    "using Printf, BSON\n",
    "using Parameters: @with_kw\n",
    "using CUDAapi\n",
    "if has_cuda()\n",
    "    @info \"CUDA is on\"\n",
    "    import CuArrays\n",
    "    CuArrays.allowscalar(false)\n",
    "end\n",
    "\n",
    "@with_kw mutable struct Args\n",
    "    lr::Float64 = 3e-3\n",
    "    epochs::Int = 20\n",
    "    batch_size = 128\n",
    "    savepath::String = \"./\" \n",
    "end\n",
    "\n",
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "function get_processed_data(args)\n",
    "    # Load labels and images from Flux.Data.MNIST\n",
    "    train_labels = MNIST.labels()\n",
    "    train_imgs = MNIST.images()\n",
    "    mb_idxs = partition(1:length(train_imgs), args.batch_size)\n",
    "    train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs] \n",
    "    \n",
    "    # Prepare test set as one giant minibatch:\n",
    "    test_imgs = MNIST.images(:test)\n",
    "    test_labels = MNIST.labels(:test)\n",
    "    test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "end\n",
    "\n",
    "# Build model\n",
    "function build_model(args; imgsize = (28,28,1), nclasses = 10)\n",
    "    cnn_output_size = Int.(floor.([imgsize[1]/8,imgsize[2]/8,32]))\t\n",
    "\n",
    "    return Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), imgsize[3]=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one using `Flux.flatten`, at this point it should be (3, 3, 32, N)\n",
    "    flatten,\n",
    "    Dense(prod(cnn_output_size), 10))\n",
    "end\n",
    "\n",
    "# We augment `x` a little bit here, adding in random noise. \n",
    "augment(x) = x .+ gpu(0.1f0*randn(eltype(x), size(x)))\n",
    "\n",
    "# Returns a vector of all parameters used in model\n",
    "paramvec(m) = vcat(map(p->reshape(p, :), params(m))...)\n",
    "\n",
    "# Function to check if any element is NaN or not\n",
    "anynan(x) = any(isnan.(x))\n",
    "\n",
    "accuracy(x, y, model) = mean(onecold(cpu(model(x))) .== onecold(cpu(y)))\n",
    "\n",
    "function train(; kws...)\t\n",
    "    args = Args(; kws...)\n",
    "\n",
    "    @info(\"Loading data set\")\n",
    "    train_set, test_set = get_processed_data(args)\n",
    "\n",
    "    # Define our model.  We will use a simple convolutional architecture with\n",
    "    # three iterations of Conv -> ReLU -> MaxPool, followed by a final Dense layer.\n",
    "    @info(\"Building model...\")\n",
    "    model = build_model(args) \n",
    "\n",
    "    # Load model and datasets onto GPU, if enabled\n",
    "    train_set = gpu.(train_set)\n",
    "    test_set = gpu.(test_set)\n",
    "    model = gpu(model)\n",
    "    \n",
    "    # Make sure our model is nicely precompiled before starting our training loop\n",
    "    model(train_set[1][1])\n",
    "\n",
    "    # `loss()` calculates the crossentropy loss between our prediction `y_hat`\n",
    "    # (calculated from `model(x)`) and the ground truth `y`.  We augment the data\n",
    "    # a bit, adding gaussian random noise to our image to make it more robust.\n",
    "    function loss(x, y)    \n",
    "        x̂ = augment(x)\n",
    "        ŷ = model(x̂)\n",
    "        return logitcrossentropy(ŷ, y)\n",
    "    end\n",
    "\t\n",
    "    # Train our model with the given training set using the ADAM optimizer and\n",
    "    # printing out performance against the test set as we go.\n",
    "    opt = ADAM(args.lr)\n",
    "\t\n",
    "    @info(\"Beginning training loop...\")\n",
    "    best_acc = 0.0\n",
    "    last_improvement = 0\n",
    "    for epoch_idx in 1:args.epochs\n",
    "        # Train for a single epoch\n",
    "        Flux.train!(loss, params(model), train_set, opt)\n",
    "\t    \n",
    "        # Terminate on NaN\n",
    "        if anynan(paramvec(model))\n",
    "            @error \"NaN params\"\n",
    "            break\n",
    "        end\n",
    "\t\n",
    "        # Calculate accuracy:\n",
    "        acc = accuracy(test_set..., model)\n",
    "\t\t\n",
    "        @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "        # If our accuracy is good enough, quit out.\n",
    "        if acc >= 0.999\n",
    "            @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "            break\n",
    "        end\n",
    "\t\n",
    "        # If this is the best accuracy we've seen so far, save the model out\n",
    "        if acc >= best_acc\n",
    "            @info(\" -> New best accuracy! Saving model out to mnist_conv.bson\")\n",
    "            BSON.@save joinpath(args.savepath, \"mnist_conv.bson\") params=cpu.(params(model)) epoch_idx acc\n",
    "            best_acc = acc\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "\t\n",
    "        # If we haven't seen improvement in 5 epochs, drop our learning rate:\n",
    "        if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "            opt.eta /= 10.0\n",
    "            @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "   \n",
    "            # After dropping learning rate, give it a few epochs to improve\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "\t\n",
    "        if epoch_idx - last_improvement >= 10\n",
    "            @warn(\" -> We're calling this converged.\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Testing the model, from saved model\n",
    "function test(; kws...)\n",
    "    args = Args(; kws...)\n",
    "    \n",
    "    # Loading the test data\n",
    "    _,test_set = get_processed_data(args)\n",
    "    \n",
    "    # Re-constructing the model with random initial weights\n",
    "    model = build_model(args)\n",
    "    \n",
    "    # Loading the saved parameters\n",
    "    BSON.@load joinpath(args.savepath, \"mnist_conv.bson\") params\n",
    "    \n",
    "    # Loading parameters onto the model\n",
    "    Flux.loadparams!(model, params)\n",
    "    \n",
    "    test_set = gpu.(test_set)\n",
    "    model = gpu(model)\n",
    "    @show accuracy(test_set...,model)\n",
    "end\n",
    "\n",
    "cd(@__DIR__) \n",
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
